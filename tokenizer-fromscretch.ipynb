{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Why large language models break down for computation**\nnormaly LLM takes process over the tokens how the model is tokenizing string is where it all comes down at the end so for mathmetical operation like:\n24 + 433\nso different models might create different tokens for this string  \neg 1 : '24' ,  ' +'   ,' 4'   ,' 33'\nalso for stings also 'hello how are you tokenization'\ntokena 'hello', ' how',' are',' you',' token','ization'\n\n\n\n****remember the space before each token it might differe model to model****\n\n\n\nanother example :\nEgg. -------->  'E' ,'gg'<br>\nI eat Egg.  ---> 'I',' eat','Egg.'<br>\nsee how same Egg. getting broken down to two when used with 'Space'\n\nhttps://tiktokenizer.vercel.app/\n\n","metadata":{}},{"cell_type":"code","source":"# UTF-8\nlist(\"hello GPT\".encode(\"utf-8\"))","metadata":{"execution":{"iopub.status.busy":"2024-02-24T09:55:05.241608Z","iopub.execute_input":"2024-02-24T09:55:05.241994Z","iopub.status.idle":"2024-02-24T09:55:05.255424Z","shell.execute_reply.started":"2024-02-24T09:55:05.241963Z","shell.execute_reply":"2024-02-24T09:55:05.254523Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"[104, 101, 108, 108, 111, 32, 71, 80, 84]"},"metadata":{}}]},{"cell_type":"markdown","source":"**LLM models can be built tokenization free by directly feeding byte streeem though the it may not be fesiable becuase now transformer has to consider large context length**\n\n\n# for to use unicode with LLM we use byte pair Encoding\nit applies like most repeated pair will create new token,and thus our vocab size will increase but context length decrease\n\n'aabbaabaac'\n\nmost repeated char 'aa' -> Z\nnow we have string as 'ZbbZbZc' vocab({Z,b,c,a}, 4)\n\n'bb' -> Y\nnow we have string as 'ZYZbZc' vocab({Z,b,c,Y,a}, 5)\n\n\n\n","metadata":{}},{"cell_type":"code","source":"text='''although that term isn’t quite right—are called code points. Code points are identified by number, customarily written in hexadecimal with the prefix “U+”, such as U+0041 “A” latin capital letter a or U+03B8 “θ” greek small letter theta. Each code point also has a short name, and quite a few other properties, specified in the Unicode Character Database.'''\nlist_uni=list(text.encode(\"utf-8\"))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-24T09:55:06.286601Z","iopub.execute_input":"2024-02-24T09:55:06.287322Z","iopub.status.idle":"2024-02-24T09:55:06.292339Z","shell.execute_reply.started":"2024-02-24T09:55:06.287289Z","shell.execute_reply":"2024-02-24T09:55:06.291235Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# now we find the most common repeated pair in byte list\ndef get_count(_list):\n    counts={}\n    for pair in zip(_list,_list[1:]):\n        counts[pair]=counts.get(pair,0)+1 \n    return counts\n","metadata":{"execution":{"iopub.status.busy":"2024-02-24T09:55:06.588037Z","iopub.execute_input":"2024-02-24T09:55:06.588386Z","iopub.status.idle":"2024-02-24T09:55:06.593561Z","shell.execute_reply.started":"2024-02-24T09:55:06.588358Z","shell.execute_reply":"2024-02-24T09:55:06.592653Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"count_dict=get_count(list_uni)\n# count_dict=sorted(((v,k) for k,v in count_dict.items()),reverse=True)\nchr(101),chr(32),chr(226), chr(128)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T09:55:06.905382Z","iopub.execute_input":"2024-02-24T09:55:06.906114Z","iopub.status.idle":"2024-02-24T09:55:06.913156Z","shell.execute_reply.started":"2024-02-24T09:55:06.906073Z","shell.execute_reply":"2024-02-24T09:55:06.912177Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"('e', ' ', 'â', '\\x80')"},"metadata":{}}]},{"cell_type":"code","source":"def merge(_list,pair,idx):\n    new_ids=[]\n    i=0\n    while i<len(_list):\n        if i<len(_list)-1 and _list[i]==pair[0] and _list[i+1]==pair[1]:\n            new_ids.append(idx)\n            i+=2\n        else:\n            new_ids.append(_list[i])\n            i+=1\n            \n    return new_ids","metadata":{"execution":{"iopub.status.busy":"2024-02-24T09:55:06.983034Z","iopub.execute_input":"2024-02-24T09:55:06.983360Z","iopub.status.idle":"2024-02-24T09:55:06.989265Z","shell.execute_reply.started":"2024-02-24T09:55:06.983334Z","shell.execute_reply":"2024-02-24T09:55:06.988396Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"top_pair=max(count_dict,key=count_dict.get)\nnew_tokens=merge(list_uni,top_pair,256) # replace top repeating pair with new 256 token\nlen(new_tokens),len(list_uni)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T09:55:07.169082Z","iopub.execute_input":"2024-02-24T09:55:07.169426Z","iopub.status.idle":"2024-02-24T09:55:07.176113Z","shell.execute_reply.started":"2024-02-24T09:55:07.169400Z","shell.execute_reply":"2024-02-24T09:55:07.175208Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(362, 372)"},"metadata":{}}]},{"cell_type":"code","source":"text='''We’ve seen that Unicode code points are abstractly identified by their index in the codespace, ranging from U+0000 to U+10FFFF. But how do code points get represented as bytes, in memory or in a fileThe most convenient, computer-friendliest (and programmer-friendliest) thing to do would be to just store the code point index as a 32-bit integer. This works, but it consumes 4 bytes per code point, which is sort of a lot. Using 32-bit ints for Unicode will cost you a bunch of extra storage, memory, and performance in bandwidth-bound scenarios, if you work with a lot of text.Consequently, there are several more-compact encodings for Unicode. The 32-bit integer encoding is officially called UTF-32 (UTF = “Unicode Transformation Format”), but it’s rarely used for storage. At most, it comes up sometimes as a temporary internal representation, for examining or operating on the code points in a string.Much more commonly, you’ll see Unicode text encoded as either UTF-8 or UTF-16. These are both variable-length encodings, made up of 8-bit or 16-bit units, respectively. In these schemes, code points with smaller index values take up fewer bytes, which saves a lot of memory for typical texts. The trade-off is that processing UTF-8/16 texts is more programmatically involved, and likely slower.UTF-8In UTF-8, each code point is stored using 1 to 4 bytes, based on its index value.UTF-8 uses a system of binary prefixes, in which the high bits of each byte mark whether it’s a single byte, the beginning of a multi-byte sequence, or a continuation byte; the remaining bits, concatenated, give the code point index. This table shows how it works:UTF-8 (binary)\tCode point (binary)\tRange0xxxxxxx\txxxxxxx\tU+0000–U+007F110xxxxx 10yyyyyy\txxxxxyyyyyy\tU+0080–U+07FF1110xxxx 10yyyyyy 10zzzzzz\txxxxyyyyyyzzzzzz\tU+0800–U+FFFF11110xxx 10yyyyyy 10zzzzzz 10wwwwww\txxxyyyyyyzzzzzzwwwwww\tU+10000–U+10FFFFA handy property of UTF-8 is that code points below 128 (ASCII characters) are encoded as single bytes, and all non-ASCII code points are encoded using sequences of bytes 128–255. This has a couple of nice consequences. First, any strings or files out there that are already in ASCII can also be interpreted as UTF-8 without any conversion. Second, lots of widely-used string programming idioms—such as null termination, or delimiters (newlines, tabs, commas, slashes, etc.)—will just work on UTF-8 strings. ASCII bytes never occur inside the encoding of non-ASCII code points, so searching byte-wise for a null terminator or a delimiter will do the right thing.Thanks to this convenience, it’s relatively simple to extend legacy ASCII programs and APIs to handle UTF-8 strings. UTF-8 is very widely used in the Unix/Linux and Web worlds, and many programmers argue UTF-8 should be the default encoding everywhere.However, UTF-8 isn’t a drop-in replacement for ASCII strings in all respects. For instance, code that iterates over the “characters” in a string will need to decode UTF-8 and iterate over code points (or maybe grapheme clusters—more about those later), not bytes. When you measure the “length” of a string, you’ll need to think about whether you want the length in bytes, the length in code points, the width of the text when rendered, or something else.UTF-16The other encoding that you’re likely to encounter is UTF-16. It uses 16-bit words, with each code point stored as either 1 or 2 words.Like UTF-8, we can express the UTF-16 encoding rules in the form of binary prefixes:UTF-16 (binary)\tCode point (binary)\tRangexxxxxxxxxxxxxxxx\txxxxxxxxxxxxxxxx\tU+0000–U+FFFF110110xxxxxxxxxx 110111yyyyyyyyyy\txxxxxxxxxxyyyyyyyyyy + 0x10000\tU+10000–U+10FFFFA more common way that people talk about UTF-16 encoding, though, is in terms of code points called “surrogates”. All the code points in the range U+D800–U+DFFF—or in other words, the code points that match the binary prefixes 110110 and 110111 in the table above—are reserved specifically for UTF-16 encoding, and don’t represent any valid characters on their own. They’re only meant to occur in the 2-word encoding pattern above, which is called a “surrogate pair”. Surrogate code points are illegal in any other context! They’re not allowed in UTF-8 or UTF-32 at all.'''\nlist_uni=list(text.encode(\"utf-8\"))\nlen(list_uni)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T09:55:07.367858Z","iopub.execute_input":"2024-02-24T09:55:07.368413Z","iopub.status.idle":"2024-02-24T09:55:07.377676Z","shell.execute_reply.started":"2024-02-24T09:55:07.368385Z","shell.execute_reply":"2024-02-24T09:55:07.376770Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"4294"},"metadata":{}}]},{"cell_type":"code","source":"def get_count(_list):\n    counts={}\n    for pair in zip(_list,_list[1:]):\n        counts[pair]=counts.get(pair,0)+1 \n    return counts\n\ndef merge(_list,pair,idx):\n    new_ids=[]\n    i=0\n    while i<len(_list):\n        if i<len(_list)-1 and _list[i]==pair[0] and _list[i+1]==pair[1]:\n            new_ids.append(idx)\n            i+=2\n        else:\n            new_ids.append(_list[i])\n            i+=1\n            \n    return new_ids\n\n###############################################################\nvocab_size=276\nnew_tokens=276-256\nlist_uni_copy=list(list_uni)\n\nmerges={}\nfor i in range(new_tokens):\n    uni_count=get_count(list_uni_copy)\n    top_uni=max(uni_count,key=uni_count.get)\n    print(f\"merging {top_uni} to {256+i}\")\n    list_uni_copy=merge(list_uni_copy,top_uni,256+i)\n    merges[top_uni]=256+i\n    ","metadata":{"execution":{"iopub.status.busy":"2024-02-24T09:55:07.604126Z","iopub.execute_input":"2024-02-24T09:55:07.604477Z","iopub.status.idle":"2024-02-24T09:55:07.688125Z","shell.execute_reply.started":"2024-02-24T09:55:07.604441Z","shell.execute_reply":"2024-02-24T09:55:07.686998Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"merging (101, 32) to 256\nmerging (105, 110) to 257\nmerging (120, 120) to 258\nmerging (115, 32) to 259\nmerging (116, 104) to 260\nmerging (99, 111) to 261\nmerging (116, 32) to 262\nmerging (111, 114) to 263\nmerging (44, 32) to 264\nmerging (101, 114) to 265\nmerging (121, 121) to 266\nmerging (101, 110) to 267\nmerging (261, 100) to 268\nmerging (100, 32) to 269\nmerging (226, 128) to 270\nmerging (257, 103) to 271\nmerging (97, 110) to 272\nmerging (258, 258) to 273\nmerging (121, 32) to 274\nmerging (116, 101) to 275\n","output_type":"stream"}]},{"cell_type":"code","source":"len(list_uni_copy),len(list_uni),((len(list_uni)-len(list_uni_copy)) / len(list_uni))","metadata":{"execution":{"iopub.status.busy":"2024-02-24T09:55:07.822120Z","iopub.execute_input":"2024-02-24T09:55:07.822851Z","iopub.status.idle":"2024-02-24T09:55:07.830242Z","shell.execute_reply.started":"2024-02-24T09:55:07.822806Z","shell.execute_reply":"2024-02-24T09:55:07.829389Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(3333, 4294, 0.22380065207265953)"},"metadata":{}}]},{"cell_type":"markdown","source":"Tokenizers has diferent dataset then the LLM models and these are trained on vocublary using \"**byte pair encoding**\" (**BPE**)\n\n\n\n\n# utf-8 must follow following format for byte representation\n# \n# 0XXXXXXX or 110XXXXX or 1110XXXX or 11110XXX\n\n128 doest follow the utf standard because 10000000\n\n\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"vocab={id:bytes([id]) for id in range(256)}\n\nfor (p0,p1),k in merges.items():\n    vocab[k]=vocab[p0]+vocab[p1]\n    \ndef decode(ids):\n    tokens=b\"\".join(vocab[id] for id in ids)\n    text=tokens.decode(\"utf-8\",errors='replace')\n    return text\n","metadata":{"execution":{"iopub.status.busy":"2024-02-24T09:55:09.242297Z","iopub.execute_input":"2024-02-24T09:55:09.242999Z","iopub.status.idle":"2024-02-24T09:55:09.248728Z","shell.execute_reply.started":"2024-02-24T09:55:09.242966Z","shell.execute_reply":"2024-02-24T09:55:09.247692Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# 97  ---->  bytes(97)  ---->  'a'\n\n# 256  ---->  bytes(256)  ---->  'e  '\n\n# 265  ---->  bytes(265)  ---->  'er'","metadata":{}},{"cell_type":"code","source":"def encode(text):\n    tokens=list(text.encode('utf-8'))\n    while len(tokens)>=2:\n        token_dict=get_count(tokens)\n        pair= min(token_dict,key=lambda p:merges.get(p,float('inf'))) \n        #float('inf') is used to make sure we don't include those pair which are not in our merge\n        if pair not in merges:\n            break #nothing to merge  \n        idx=merges[pair]\n        tokens=merge(tokens,pair,idx)\n    return tokens","metadata":{"execution":{"iopub.status.busy":"2024-02-24T09:55:10.445125Z","iopub.execute_input":"2024-02-24T09:55:10.445835Z","iopub.status.idle":"2024-02-24T09:55:10.451741Z","shell.execute_reply.started":"2024-02-24T09:55:10.445802Z","shell.execute_reply":"2024-02-24T09:55:10.450854Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print(decode(encode('hello world!')))","metadata":{"execution":{"iopub.status.busy":"2024-02-24T09:55:11.155769Z","iopub.execute_input":"2024-02-24T09:55:11.156550Z","iopub.status.idle":"2024-02-24T09:55:11.161032Z","shell.execute_reply.started":"2024-02-24T09:55:11.156522Z","shell.execute_reply":"2024-02-24T09:55:11.160146Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"hello world!\n","output_type":"stream"}]},{"cell_type":"code","source":"decode([263])","metadata":{"execution":{"iopub.status.busy":"2024-02-24T09:55:11.961242Z","iopub.execute_input":"2024-02-24T09:55:11.962084Z","iopub.status.idle":"2024-02-24T09:55:11.967659Z","shell.execute_reply.started":"2024-02-24T09:55:11.962048Z","shell.execute_reply":"2024-02-24T09:55:11.966685Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'or'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Forced spilt using regex pattern(GPT series)","metadata":{}},{"cell_type":"code","source":"import regex as re\ngpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n\n# gpt4pat=r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n# i for case insenstive\n\n# \" ?\\p{L}\" --->               ' ?'optional space followed by p{L} which means letter of anykind+ for following e.g it is enough\n# \" ?\\p{N}\"--->                any neumeric character\n# \" ?[^\\s\\p{L}\\p{N}]+\" --->    group special char\n# \"\\s+(?!\\S)\"         --->     include alll white spaces in seq but not last one '(?!\\S)'\\\n#  \"       dog\" -------------->      '       ', ' dog'\n\nre.findall(gpt2pat,\"hellow i've dog!???        dog123    \")","metadata":{"execution":{"iopub.status.busy":"2024-02-24T10:25:26.004137Z","iopub.execute_input":"2024-02-24T10:25:26.004530Z","iopub.status.idle":"2024-02-24T10:25:26.011872Z","shell.execute_reply.started":"2024-02-24T10:25:26.004499Z","shell.execute_reply":"2024-02-24T10:25:26.011016Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"['hellow', ' i', \"'ve\", ' dog', '!???', '       ', ' dog', '123', '    ']"},"metadata":{}}]},{"cell_type":"code","source":"# !pip install tiktoken\nimport tiktoken\n\n# gpt2\nenc=tiktoken.get_encoding(\"gpt2\")\nprint(enc.encode(\"     what is the status\"))\n\nenc=tiktoken.get_encoding(\"cl100k_base\")\nprint(enc.encode(\"     what is the status3456\"))","metadata":{"execution":{"iopub.status.busy":"2024-02-24T10:39:48.905029Z","iopub.execute_input":"2024-02-24T10:39:48.905424Z","iopub.status.idle":"2024-02-24T10:39:48.911542Z","shell.execute_reply.started":"2024-02-24T10:39:48.905394Z","shell.execute_reply":"2024-02-24T10:39:48.910663Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"[220, 220, 220, 220, 644, 318, 262, 3722]\n[257, 1148, 374, 279, 2704, 12901, 21]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# SentencePiece\n----------------------------------------\nThe big difference between tiktoken vs sentencepiece is sentencepiece runs BPE on the Unicode code points directly .it then has an option character_coverage for what to do with very very rare codepoints that apperar very few times, and it either map them to <UNK> or if byte_fallback is turned on it encodes them with utf-8 and then encoedes the raw bytes instead **","metadata":{}},{"cell_type":"code","source":"import sentencepiece as spm","metadata":{"execution":{"iopub.status.busy":"2024-02-24T11:34:45.097201Z","iopub.execute_input":"2024-02-24T11:34:45.097557Z","iopub.status.idle":"2024-02-24T11:34:45.101576Z","shell.execute_reply.started":"2024-02-24T11:34:45.097527Z","shell.execute_reply":"2024-02-24T11:34:45.100742Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"import os \noptions=dict(\n    input=\"\"\"The phrase \"problem solved itself\" is often used figuratively to describe situations where a problem appears to have been resolved without direct intervention or effort from individuals involved. However, in reality, there's usually some underlying mechanism or action that led to the resolution, even if it's not immediately obvious.Here are a few ways a problem might appear to solve itself: Natural Processes: Sometimes, problems may seem to resolve on their own due to natural processes or phenomena. For example, if there's a conflict between two parties and one party unexpectedly decides to withdraw, the problem may appear to have resolved itself, but it's actually due to a decision made by one of the parties. Time: Some problems may resolve over time without direct intervention. This could be due to changing circumstances, attitudes, or perspectives. For instance, a disagreement between coworkers might gradually dissipate as they spend more time working together and gaining a better understanding of each other's viewpoints. Random Chance: In some cases, random chance or luck might seemingly resolve a problem. For instance, if there's a technical issue with a computer system that suddenly corrects itself without any intervention, it might appear as though the problem solved itself, when in reality, it was just a temporary glitch. Unintended Consequences: Occasionally, actions taken to address a problem can inadvertently lead to its resolution through unintended consequences. For example, implementing a new policy to address one issue might inadvertently resolve another unrelated problem. It's important to recognize that while a problem may seem to have solved itself, there are usually underlying factors at play, even if they're not immediately apparent. In many cases, understanding these underlying factors can help prevent similar problems from arising in the future or provide insights into more effective problem-solving strategies. User how does problem solved it self ChatGPT In some cases, a problem may appear to solve itself due to various factors. Here are a few ways this can happen:Self-Correction: Sometimes, systems have built-in mechanisms that automatically correct issues. For instance, in a biological system, the body's immune system can often resolve infections without external intervention. Similarly, in a technical system, error-detection and error-correction mechanisms can sometimes fix problems automatically.External Factors: Changes in external circumstances can sometimes resolve a problem without any direct action. For example, if a business is struggling due to economic downturn, an improvement in the economy might lead to increased sales and revenue, seemingly resolving the problem without any specific action taken by the business.Spontaneous Resolution: Some problems may simply resolve on their own over time due to natural fluctuations or random chance. For example, conflicts between individuals or groups might gradually dissipate as emotions cool down or as people's priorities shift.Learning and Adaptation: Individuals or systems involved in a problem may learn from their experiences and adapt their behavior accordingly, leading to a resolution over time. For instance, if a software program encounters a bug that causes it to crash, developers might observe and analyze the issue, then release a patch that fixes the bug, seemingly resolving the problem without direct intervention from users.Complex Systems Dynamics: In complex systems, problems can sometimes be self-organizing or self-regulating. Emergent properties or feedback loops within the system can lead to problem-solving behaviors without any centralized control. For example, in ecological systems, predator-prey dynamics can naturally regulate populations without external intervention.While it may seem like a problem has solved itself, it's often the result of underlying processes or changes occurring within the system. Recognizing these mechanisms can help us understand how problems are resolved and how to facilitate more effective problem-solving in the future.\"\"\",\n    input_format=\"text\",\n#     model_prefix=\"tok100\", #outputfilename\n    model_type=\"bpe\",\n    vocab_size=100,\n    normalization_rule_name=\"identity\",\n    remove_extra_whitespaces=False,\n    input_sentence_size=1000000,\n    shuffle_input_sentence=True,\n    character_coverage=0.9995,\n    byte_fallback=True,\n    unk_id=0,\n    bos_id=1,\n    eos_id=2,\n    pad_id=-1,\n    num_threads=os.cpu_count()\n)\nspm.SentencePieceTrainer.Train(**options)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T11:36:53.352972Z","iopub.execute_input":"2024-02-24T11:36:53.353298Z","iopub.status.idle":"2024-02-24T11:36:53.414683Z","shell.execute_reply.started":"2024-02-24T11:36:53.353274Z","shell.execute_reply":"2024-02-24T11:36:53.413245Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stderr","text":"sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \ntrainer_spec {\n  input: The phrase \"problem solved itself\" is often used figuratively to describe situations where a problem appears to have been resolved without direct intervention or effort from individuals involved. However\n  input:  in reality\n  input:  there's usually some underlying mechanism or action that led to the resolution\n  input:  even if it's not immediately obvious.Here are a few ways a problem might appear to solve itself: Natural Processes: Sometimes\n  input:  problems may seem to resolve on their own due to natural processes or phenomena. For example\n  input:  if there's a conflict between two parties and one party unexpectedly decides to withdraw\n  input:  the problem may appear to have resolved itself\n  input:  but it's actually due to a decision made by one of the parties. Time: Some problems may resolve over time without direct intervention. This could be due to changing circumstances\n  input:  attitudes\n  input:  or perspectives. For instance\n  input:  a disagreement between coworkers might gradually dissipate as they spend more time working together and gaining a better understanding of each other's viewpoints. Random Chance: In some cases\n  input:  random chance or luck might seemingly resolve a problem. For instance\n  input:  if there's a technical issue with a computer system that suddenly corrects itself without any intervention\n  input:  it might appear as though the problem solved itself\n  input:  when in reality\n  input:  it was just a temporary glitch. Unintended Consequences: Occasionally\n  input:  actions taken to address a problem can inadvertently lead to its resolution through unintended consequences. For example\n  input:  implementing a new policy to address one issue might inadvertently resolve another unrelated problem. It's important to recognize that while a problem may seem to have solved itself\n  input:  there are usually underlying factors at play\n  input:  even if they're not immediately apparent. In many cases\n  input:  understanding these underlying factors can help prevent similar problems from arising in the future or provide insights into more effective problem-solving strategies. User how does problem solved it self ChatGPT In some cases\n  input:  a problem may appear to solve itself due to various factors. Here are a few ways this can happen:Self-Correction: Sometimes\n  input:  systems have built-in mechanisms that automatically correct issues. For instance\n  input:  in a biological system\n  input:  the body's immune system can often resolve infections without external intervention. Similarly\n  input:  in a technical system\n  input:  error-detection and error-correction mechanisms can sometimes fix problems automatically.External Factors: Changes in external circumstances can sometimes resolve a problem without any direct action. For example\n  input:  if a business is struggling due to economic downturn\n  input:  an improvement in the economy might lead to increased sales and revenue\n  input:  seemingly resolving the problem without any specific action taken by the business.Spontaneous Resolution: Some problems may simply resolve on their own over time due to natural fluctuations or random chance. For example\n  input:  conflicts between individuals or groups might gradually dissipate as emotions cool down or as people's priorities shift.Learning and Adaptation: Individuals or systems involved in a problem may learn from their experiences and adapt their behavior accordingly\n  input:  leading to a resolution over time. For instance\n  input:  if a software program encounters a bug that causes it to crash\n  input:  developers might observe and analyze the issue\n  input:  then release a patch that fixes the bug\n  input:  seemingly resolving the problem without direct intervention from users.Complex Systems Dynamics: In complex systems\n  input:  problems can sometimes be self-organizing or self-regulating. Emergent properties or feedback loops within the system can lead to problem-solving behaviors without any centralized control. For example\n  input:  in ecological systems\n  input:  predator-prey dynamics can naturally regulate populations without external intervention.While it may seem like a problem has solved itself\n  input:  it's often the result of underlying processes or changes occurring within the system. Recognizing these mechanisms can help us understand how problems are resolved and how to facilitate more effective problem-solving in the future.\n  input_format: text\n  model_prefix: \n  model_type: BPE\n  vocab_size: 100\n  self_test_sample_size: 0\n  character_coverage: 0.9995\n  input_sentence_size: 1000000\n  shuffle_input_sentence: 1\n  seed_sentencepiece_size: 1000000\n  shrinking_factor: 0.75\n  max_sentence_length: 4192\n  num_threads: 4\n  num_sub_iterations: 2\n  max_sentencepiece_length: 16\n  split_by_unicode_script: 1\n  split_by_number: 1\n  split_by_whitespace: 1\n  split_digits: 0\n  pretokenization_delimiter: \n  treat_whitespace_as_suffix: 0\n  allow_whitespace_only_pieces: 0\n  required_chars: \n  byte_fallback: 1\n  vocabulary_output_piece_score: 1\n  train_extremely_large_corpus: 0\n  hard_vocab_limit: 1\n  use_all_vocab: 0\n  unk_id: 0\n  bos_id: 1\n  eos_id: 2\n  pad_id: -1\n  unk_piece: <unk>\n  bos_piece: <s>\n  eos_piece: </s>\n  pad_piece: <pad>\n  unk_surface:  ⁇ \n  enable_differential_privacy: 0\n  differential_privacy_noise_level: 0\n  differential_privacy_clipping_threshold: 0\n}\nnormalizer_spec {\n  name: identity\n  add_dummy_prefix: 1\n  remove_extra_whitespaces: 0\n  escape_whitespaces: 1\n  normalization_rule_tsv: \n}\ndenormalizer_spec {}\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[54], line 20\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \n\u001b[1;32m      2\u001b[0m options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mThe phrase \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproblem solved itself\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is often used figuratively to describe situations where a problem appears to have been resolved without direct intervention or effort from individuals involved. However, in reality, there\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms usually some underlying mechanism or action that led to the resolution, even if it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms not immediately obvious.Here are a few ways a problem might appear to solve itself: Natural Processes: Sometimes, problems may seem to resolve on their own due to natural processes or phenomena. For example, if there\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms a conflict between two parties and one party unexpectedly decides to withdraw, the problem may appear to have resolved itself, but it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms actually due to a decision made by one of the parties. Time: Some problems may resolve over time without direct intervention. This could be due to changing circumstances, attitudes, or perspectives. For instance, a disagreement between coworkers might gradually dissipate as they spend more time working together and gaining a better understanding of each other\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms viewpoints. Random Chance: In some cases, random chance or luck might seemingly resolve a problem. For instance, if there\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms a technical issue with a computer system that suddenly corrects itself without any intervention, it might appear as though the problem solved itself, when in reality, it was just a temporary glitch. Unintended Consequences: Occasionally, actions taken to address a problem can inadvertently lead to its resolution through unintended consequences. For example, implementing a new policy to address one issue might inadvertently resolve another unrelated problem. It\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms important to recognize that while a problem may seem to have solved itself, there are usually underlying factors at play, even if they\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre not immediately apparent. In many cases, understanding these underlying factors can help prevent similar problems from arising in the future or provide insights into more effective problem-solving strategies. User how does problem solved it self ChatGPT In some cases, a problem may appear to solve itself due to various factors. Here are a few ways this can happen:Self-Correction: Sometimes, systems have built-in mechanisms that automatically correct issues. For instance, in a biological system, the body\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms immune system can often resolve infections without external intervention. Similarly, in a technical system, error-detection and error-correction mechanisms can sometimes fix problems automatically.External Factors: Changes in external circumstances can sometimes resolve a problem without any direct action. For example, if a business is struggling due to economic downturn, an improvement in the economy might lead to increased sales and revenue, seemingly resolving the problem without any specific action taken by the business.Spontaneous Resolution: Some problems may simply resolve on their own over time due to natural fluctuations or random chance. For example, conflicts between individuals or groups might gradually dissipate as emotions cool down or as people\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms priorities shift.Learning and Adaptation: Individuals or systems involved in a problem may learn from their experiences and adapt their behavior accordingly, leading to a resolution over time. For instance, if a software program encounters a bug that causes it to crash, developers might observe and analyze the issue, then release a patch that fixes the bug, seemingly resolving the problem without direct intervention from users.Complex Systems Dynamics: In complex systems, problems can sometimes be self-organizing or self-regulating. Emergent properties or feedback loops within the system can lead to problem-solving behaviors without any centralized control. For example, in ecological systems, predator-prey dynamics can naturally regulate populations without external intervention.While it may seem like a problem has solved itself, it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms often the result of underlying processes or changes occurring within the system. Recognizing these mechanisms can help us understand how problems are resolved and how to facilitate more effective problem-solving in the future.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     input_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     num_threads\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mcpu_count()\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 20\u001b[0m \u001b[43mspm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentencePieceTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sentencepiece/__init__.py:989\u001b[0m, in \u001b[0;36mSentencePieceTrainer.Train\u001b[0;34m(arg, logstream, **kwargs)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mTrain\u001b[39m(arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, logstream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    988\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m _LogStream(ostream\u001b[38;5;241m=\u001b[39mlogstream):\n\u001b[0;32m--> 989\u001b[0m     \u001b[43mSentencePieceTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Train\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sentencepiece/__init__.py:982\u001b[0m, in \u001b[0;36mSentencePieceTrainer._Train\u001b[0;34m(arg, **kwargs)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SentencePieceTrainer\u001b[38;5;241m.\u001b[39m_TrainFromMap2(new_kwargs, sentence_iterator)\n\u001b[1;32m    981\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 982\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSentencePieceTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_TrainFromMap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sentencepiece/__init__.py:927\u001b[0m, in \u001b[0;36mSentencePieceTrainer._TrainFromMap\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_TrainFromMap\u001b[39m(args):\n\u001b[0;32m--> 927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sentencepiece\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentencePieceTrainer__TrainFromMap\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: Internal: src/trainer_interface.cc(335) [(output_model_proto_ != nullptr && trainer_spec_.model_prefix().empty()) || (output_model_proto_ == nullptr && !trainer_spec_.model_prefix().empty())] ModelProto and trainer_spec.model_prefix() must be exclusive."],"ename":"RuntimeError","evalue":"Internal: src/trainer_interface.cc(335) [(output_model_proto_ != nullptr && trainer_spec_.model_prefix().empty()) || (output_model_proto_ == nullptr && !trainer_spec_.model_prefix().empty())] ModelProto and trainer_spec.model_prefix() must be exclusive.","output_type":"error"}]},{"cell_type":"code","source":"sp=spm.SentencePieceProcessor()\nsp.load('tok400.model')\nvocab=[[sp.id_to_pieace(idx),idx] for idx in range(sp.get_piece_size())]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**why we can not create vocab_size to \"inf or 1 million\"?**\n- with increasing size of vocab there will be embeddings for each token also which must be trained , so in case of 1 million vocab_size these token will become rare to encounter in train set because so these embeddings might remain under trained,also if it is possible we train a very very huge data ,now we can cover more context with new squished tokens but model may not get time for each knowledgefull token to consider while predicting next token\n\n\n- also model may behave differently when they see 'hello how ' space at the end ,because you have added the ' ' which will be part of next token like ' are',and model has not seen more ' ' wth any text following also those token which are not part of token dataset '.DefaultCellsty' called undatable token","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}